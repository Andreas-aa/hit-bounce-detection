{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1e112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 19:27:15.032560: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-14 19:27:15.035351: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-14 19:27:15.210348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-14 19:27:20.286943: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-14 19:27:20.288406: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, f1_score, make_scorer, fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98930c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory\n",
    "path_dir = Path.cwd()\n",
    "\n",
    "# Folder with the JSON files\n",
    "json_dir = path_dir / \"per_point_v2\"\n",
    "\n",
    "# Prepare an empty DataFrame with the expected columns and an index name\n",
    "df = pd.DataFrame(columns=[\"x\", \"y\", \"visible\", \"action\"])\n",
    "df.index.name = \"image_frame\"\n",
    "\n",
    "frames_df = []\n",
    "\n",
    "for json_path in json_dir.glob(\"*.json\"):\n",
    "    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        ball_data = json.load(f)  # expected: dict keyed by image_frame\n",
    "\n",
    "    # Build a DataFrame from the JSON dict, then transpose:\n",
    "    file_df = pd.DataFrame(ball_data).T\n",
    "    file_df.index.name = \"image_frame\"\n",
    "\n",
    "    # Ensure column names match the expected schema\n",
    "    file_df = file_df.reindex(columns=[\"x\", \"y\", \"visible\", \"action\"])\n",
    "\n",
    "    frames_df.append(file_df)\n",
    "\n",
    "# Final concatenation\n",
    "df = pd.concat(frames_df, axis=0, ignore_index=False)\n",
    "df.index.name = \"image_frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991dc691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocessors.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_features(\n",
    "    subset_df: pd.DataFrame,\n",
    "    smooth_window: int = 7,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature builder for ball hit / bounce detection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Numeric positions and index\n",
    "    # ------------------------------------------------------------------\n",
    "    subset = subset_df.copy()\n",
    "    subset.index = pd.to_numeric(subset.index, errors=\"coerce\")\n",
    "    subset = subset.sort_index()\n",
    "    subset[\"x_i\"] = pd.to_numeric(subset[\"x\"], errors=\"coerce\")\n",
    "    subset[\"y_i\"] = pd.to_numeric(subset[\"y\"], errors=\"coerce\")\n",
    "    subset = subset.dropna(subset=[\"x_i\", \"y_i\"])\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw positions\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"x_raw\"] = subset[\"x_i\"]\n",
    "    subset[\"y_raw\"] = subset[\"y_i\"]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Centered smoothing on positions\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Centered rolling mean reduces high-frequency measurement noise\n",
    "    # without eliminating physical discontinuities (hits / bounces).\n",
    "    subset[\"x_smooth\"] = (\n",
    "        subset[\"x_raw\"]\n",
    "        .rolling(smooth_window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "    subset[\"y_smooth\"] = (\n",
    "        subset[\"y_raw\"]\n",
    "        .rolling(smooth_window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Time step (central)\n",
    "    # ------------------------------------------------------------------\n",
    "    t = subset.index.to_series()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Smoothed derivatives (stable kinematics)\n",
    "    # ------------------------------------------------------------------\n",
    "    x_smooth = subset[\"x_smooth\"].to_numpy()\n",
    "    y_smooth = subset[\"y_smooth\"].to_numpy()\n",
    "\n",
    "    vx = np.gradient(x_smooth, t)\n",
    "    vy = np.gradient(y_smooth, t)\n",
    "\n",
    "    ax = np.gradient(vx, t)\n",
    "    ay = np.gradient(vy, t)\n",
    "\n",
    "    jx = np.gradient(ax, t)\n",
    "    jy = np.gradient(ay, t)\n",
    "\n",
    "    subset[\"vx\"] = vx\n",
    "    subset[\"vy\"] = vy\n",
    "    subset[\"ax\"] = ax\n",
    "    subset[\"ay\"] = ay\n",
    "    subset[\"jx\"] = jx\n",
    "    subset[\"jy\"] = jy\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw derivatives (impulse-sensitive)\n",
    "    # ------------------------------------------------------------------\n",
    "    x_raw = subset[\"x_raw\"].to_numpy()\n",
    "    y_raw = subset[\"y_raw\"].to_numpy()\n",
    "\n",
    "    vx_raw = np.gradient(x_raw, t)\n",
    "    vy_raw = np.gradient(y_raw, t)\n",
    "\n",
    "    ax_raw = np.gradient(vx_raw, t)\n",
    "    ay_raw = np.gradient(vy_raw, t)\n",
    "\n",
    "    jx_raw = np.gradient(ax_raw, t)\n",
    "    jy_raw = np.gradient(ay_raw, t)\n",
    "\n",
    "    subset[\"vx_raw\"] = vx_raw\n",
    "    subset[\"vy_raw\"] = vy_raw\n",
    "    subset[\"ax_raw\"] = ax_raw\n",
    "    subset[\"ay_raw\"] = ay_raw\n",
    "    subset[\"jx_raw\"] = jx_raw\n",
    "    subset[\"jy_raw\"] = jy_raw\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw derivatubes in absolute\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    subset[\"vx_abs_raw\"] = np.abs(subset[\"vx_raw\"])\n",
    "    subset[\"vy_abs_raw\"] = np.abs(subset[\"vy_raw\"])\n",
    "    subset[\"ax_abs_raw\"] = np.abs(subset[\"ax_raw\"])\n",
    "    subset[\"ay_abs_raw\"] = np.abs(subset[\"ay_raw\"])\n",
    "    subset[\"jx_abs_raw\"] = np.abs(subset[\"jx_raw\"])\n",
    "    subset[\"jy_abs_raw\"] = np.abs(subset[\"jy_raw\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Magnitudes (smoothed)\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"v\"] = np.sqrt(subset[\"vx\"]**2 + subset[\"vy\"]**2)\n",
    "    subset[\"a\"] = np.sqrt(subset[\"ax\"]**2 + subset[\"ay\"]**2)\n",
    "    subset[\"jerk\"] = np.sqrt(subset[\"jx\"]**2 + subset[\"jy\"]**2)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Log magnitudes : preserves order and compresses large values\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"log_v\"] = np.log1p(subset[\"v\"])    \n",
    "    subset[\"log_a\"] = np.log1p(subset[\"a\"])\n",
    "    subset[\"log_j\"] = np.log1p(subset[\"jerk\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Directional features\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"angle\"] = np.arctan2(subset[\"vy\"], subset[\"vx\"])\n",
    "    subset[\"delta_angle\"] = np.gradient(subset[\"angle\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Centered rolling statistics (smoothed)\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"v_mean\"] = subset[\"v\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    subset[\"v_std\"]  = subset[\"v\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    subset[\"a_mean\"] = subset[\"a\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    subset[\"a_std\"]  = subset[\"a\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    subset[\"j_mean\"] = subset[\"jerk\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    subset[\"j_std\"]  = subset[\"jerk\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Motion sign changes\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"vx_sign\"] = np.sign(subset[\"vx\"]).fillna(0.0)\n",
    "    subset[\"vx_sign_change\"] = (\n",
    "        subset[\"vx_sign\"].diff().abs() > 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    subset[\"vy_sign\"] = np.sign(subset[\"vy\"]).fillna(0.0)\n",
    "    subset[\"vy_sign_change\"] = (\n",
    "        subset[\"vy_sign\"].diff().abs() > 0\n",
    "    ).astype(int)\n",
    "\n",
    "    return subset\n",
    "\n",
    "# Select features\n",
    "FEATURE_COLS = [\n",
    "    \"delta_angle\",\n",
    "    \"vx_sign_change\", \n",
    "    \"vy_sign_change\",\n",
    "    \"v\", \"a\", \"jerk\",\n",
    "    \"vx\", 'vy', 'ax', 'ay', 'jx', 'jy',\n",
    "    \"v_mean\", \"v_std\",\n",
    "    \"a_mean\", \"a_std\",\n",
    "    \"j_mean\", \"j_std\",\n",
    "    \"log_v\", \"log_a\", \"log_j\",\n",
    "    \"vx_abs_raw\", \"vy_abs_raw\",\n",
    "    \"ax_abs_raw\", \"ay_abs_raw\",\n",
    "    \"jx_abs_raw\", \"jy_abs_raw\",\n",
    "]\n",
    "FEATURE_COLS_DEEP = FEATURE_COLS + [\"x_i\", \"y_i\"]\n",
    "SMOOTH_WINDOW = 7\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy.index = pd.to_numeric(df_copy.index, errors=\"coerce\")\n",
    "df_copy = df_copy.sort_index()\n",
    "split_point = int(0.8 * len(df_copy))\n",
    "train_df_raw = df_copy.iloc[:split_point]\n",
    "test_df_raw  = df_copy.iloc[split_point:]\n",
    "train_df = build_features(train_df_raw, smooth_window=SMOOTH_WINDOW)\n",
    "test_df  = build_features(test_df_raw,  smooth_window=SMOOTH_WINDOW)\n",
    "\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "X_test  = test_df[FEATURE_COLS]\n",
    "X_train_deep = train_df[FEATURE_COLS_DEEP]\n",
    "X_test_deep  = test_df[FEATURE_COLS_DEEP]\n",
    "y_train = train_df[\"action\"].to_numpy()\n",
    "y_test  = test_df[\"action\"].to_numpy()\n",
    "\n",
    "# Scaling (fit on train, apply to test)\n",
    "scaler = StandardScaler()\n",
    "scaler_deep = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "X_train_deep_scaled = scaler_deep.fit_transform(X_train_deep)\n",
    "X_test_deep_scaled  = scaler_deep.transform(X_test_deep)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_labeled = le.fit_transform(y_train)\n",
    "y_test_labeled  = le.transform(y_test)\n",
    "classes = le.classes_\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Saving preprocessors\n",
    "preprocessors = {\n",
    "    \"scaler\": scaler,\n",
    "    \"scaler_deep\": scaler_deep,\n",
    "    \"label_encoder\": le\n",
    "}\n",
    "joblib.dump(preprocessors, \"preprocessors.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef770f12",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3b4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "physics_df = build_features(df_copy, smooth_window=SMOOTH_WINDOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0712fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_action_physics\n",
       "air    1550\n",
       "hit      50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_physics.loc[df_physics['action']==\"hit\", \"pred_action_physics\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a5ae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(92.17857142857142)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['ay'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0125f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "\n",
    "# ==============================\n",
    "# Estimation automatique des seuils\n",
    "# ==============================\n",
    "def estimate_heuristic_thresholds(train_df, window=3):\n",
    "    ay_abs = train_df[\"ay_abs_raw\"].values\n",
    "    ay = train_df[\"ay_raw\"].values\n",
    "    ax = train_df[\"ax_raw\"].values\n",
    "    vx = train_df[\"vx_raw\"].values\n",
    "    vy = train_df[\"vy_raw\"].values\n",
    "    jx = train_df[\"jx_raw\"].values\n",
    "    jy = train_df[\"jy_raw\"].values\n",
    "\n",
    "    jerk = np.sqrt(jx**2 + jy**2)\n",
    "\n",
    "    return {\n",
    "        \"AY_PEAK_MIN\": np.percentile(ay_abs, 70),       # seuil vertical plus permissif\n",
    "        \"PROMINENCE\": np.percentile(ay_abs, 60),       # un peu moins strict\n",
    "        \"VX_ENERGY_DELTA\": np.percentile(np.abs(np.diff(vx)), 70),\n",
    "        \"VX_MIN_MOVE\": np.percentile(np.abs(vx), 20),\n",
    "        \"JERK_THRESHOLD\": np.percentile(jerk, 70),\n",
    "        \"RATIO_VERT_HORIZ\": 1.5,\n",
    "        \"AY_CONCAVE_MAX\": np.percentile(ay, 10),\n",
    "        \"AY_VIOLENCE\": np.percentile(ay_abs, 95),\n",
    "    }\n",
    "\n",
    "# ==============================\n",
    "# Détecteur heuristique amélioré\n",
    "# ==============================\n",
    "def heuristic_event_detector(features_df, thresholds, refractory=10, window=3):\n",
    "    df = features_df.copy()\n",
    "    \n",
    "    ay_abs = df[\"ay_abs_raw\"].values\n",
    "    ay = df[\"ay_raw\"].values\n",
    "    ax = df[\"ax_raw\"].values\n",
    "    vx = df[\"vx_raw\"].values\n",
    "    vy = df[\"vy_raw\"].values\n",
    "    jx = df[\"jx_raw\"].values\n",
    "    jy = df[\"jy_raw\"].values\n",
    "    \n",
    "    jerk = np.sqrt(jx**2 + jy**2)\n",
    "\n",
    "    # Détection des pics verticaux\n",
    "    peaks, _ = find_peaks(\n",
    "        ay_abs,\n",
    "        height=thresholds[\"AY_PEAK_MIN\"],\n",
    "        prominence=thresholds[\"PROMINENCE\"],\n",
    "        distance=3\n",
    "    )\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for i in peaks:\n",
    "        if i < window or i + window >= len(df):\n",
    "            continue\n",
    "\n",
    "        vx_pre = vx[i - window]\n",
    "        vx_post = vx[i + window]\n",
    "        vy_pre = vy[i - window]\n",
    "        vy_post = vy[i + window]\n",
    "        ay_val = ay[i]\n",
    "        ax_val = ax[i]\n",
    "        jerk_val = jerk[i]\n",
    "\n",
    "        delta_vx = abs(vx_post) - abs(vx_pre)\n",
    "        max_vx = max(abs(vx_pre), abs(vx_post))\n",
    "        vx_flip = vx_pre * vx_post < 0\n",
    "        vy_flip = vy_pre * vy_post < 0\n",
    "\n",
    "        ratio_vert_horiz = abs(ay_val) / (abs(ax_val) + 1e-6)\n",
    "        score = abs(ay_val) + jerk_val + 0.5 * delta_vx  # score combiné pondéré\n",
    "\n",
    "        pred, priority = None, 0\n",
    "\n",
    "        # ----- Hit -----\n",
    "        if (\n",
    "            (vx_flip and max_vx > thresholds[\"VX_MIN_MOVE\"])\n",
    "            or (delta_vx > thresholds[\"VX_ENERGY_DELTA\"])\n",
    "            or (abs(ay_val) > thresholds[\"AY_VIOLENCE\"])\n",
    "            or (jerk_val > thresholds[\"JERK_THRESHOLD\"])\n",
    "            or (ratio_vert_horiz < thresholds[\"RATIO_VERT_HORIZ\"])\n",
    "        ):\n",
    "            pred, priority = 2, 2  # hit\n",
    "\n",
    "        # ----- Bounce -----\n",
    "        elif ay_val < thresholds[\"AY_CONCAVE_MAX\"]:  # plus souple, vy_flip non requis\n",
    "            pred, priority = 1, 1\n",
    "\n",
    "        if pred is not None:\n",
    "            candidates.append((df.index[i], pred, score, priority))\n",
    "\n",
    "    # ----- Temporal NMS souple -----\n",
    "    final = {}\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    for c in candidates:\n",
    "        if not final:\n",
    "            final[c[0]] = c[1]\n",
    "        else:\n",
    "            last_frame = list(final.keys())[-1]\n",
    "            if c[0] - last_frame >= refractory:\n",
    "                final[c[0]] = c[1]\n",
    "            else:\n",
    "                # garder l'événement le plus fort dans la fenêtre\n",
    "                last_score = [x[2] for x in candidates if x[0] == last_frame][0]\n",
    "                if c[2] > last_score:\n",
    "                    final[last_frame] = c[1]\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d14ded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temporal event evaluation (+/- 2 frames)\n",
      "----------------------------------------------------\n",
      "Event        | Precision  | Recall     | F1-Score  \n",
      "----------------------------------------------------\n",
      "Class 1      |      0.413 |      0.185 |      0.256\n",
      "Class 2      |      0.169 |      0.607 |      0.264\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Exemple d'utilisation\n",
    "# ==============================\n",
    "thresholds = estimate_heuristic_thresholds(train_df)\n",
    "heuristic_preds = heuristic_event_detector(test_df, thresholds)\n",
    "\n",
    "y_pred_heuristic = np.zeros(len(test_df), dtype=int)\n",
    "for frame, cls in heuristic_preds.items():\n",
    "    if frame in test_df.index:\n",
    "        y_pred_heuristic[test_df.index.get_loc(frame)] = cls\n",
    "\n",
    "temporal_event_eval(\n",
    "    y_test_labeled,\n",
    "    y_pred_heuristic,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a4870",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8276cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_event_eval(y_true, y_pred, tolerance=2, use_labels=True):\n",
    "    \"\"\"\n",
    "    Event-level evaluation for temporal predictions with +/- tolerance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth events (can be numeric or string labels)\n",
    "    y_pred : array-like\n",
    "        Predicted events (same format as y_true)\n",
    "    tolerance : int\n",
    "        Number of frames before/after to consider a prediction correct\n",
    "    use_labels : bool\n",
    "        If True, expects string labels like \"air\", \"bounce\", \"hit\".\n",
    "        If False, expects numeric labels like 0, 1, 2.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if use_labels:\n",
    "        event_classes = [\"bounce\", \"hit\"]\n",
    "    else:\n",
    "        event_classes = [1, 2]\n",
    "\n",
    "    print(f\"\\nTemporal event evaluation (+/- {tolerance} frames)\")\n",
    "    print(\"-\" * 52)\n",
    "    print(f\"{'Event':<12} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10}\")\n",
    "    print(\"-\" * 52)\n",
    "\n",
    "    for event in event_classes:\n",
    "        true_indices = np.where(y_true == event)[0]\n",
    "        pred_indices = np.where(y_pred == event)[0]\n",
    "\n",
    "        # ---------- Recall ----------\n",
    "        matched_true = np.array([np.any(np.abs(pred_indices - t) <= tolerance) for t in true_indices])\n",
    "        recall = matched_true.sum() / len(true_indices) if len(true_indices) > 0 else 0.0\n",
    "\n",
    "        # ---------- Precision ----------\n",
    "        matched_pred = np.array([np.any(np.abs(true_indices - p) <= tolerance) for p in pred_indices])\n",
    "        precision = matched_pred.sum() / len(pred_indices) if len(pred_indices) > 0 else 0.0\n",
    "\n",
    "        # ---------- F1 ----------\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        event_name = str(event).capitalize() if use_labels else f\"Class {event}\"\n",
    "        print(f\"{event_name:<12} | {precision:>10.3f} | {recall:>10.3f} | {f1:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938a69a",
   "metadata": {},
   "source": [
    "## 1.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d65dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 7}\n",
      "\n",
      "=== Standard Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         air       0.99      0.99      0.99     23385\n",
      "      bounce       0.69      0.64      0.66       308\n",
      "         hit       0.71      0.53      0.61       323\n",
      "\n",
      "    accuracy                           0.98     24016\n",
      "   macro avg       0.80      0.72      0.75     24016\n",
      "weighted avg       0.98      0.98      0.98     24016\n",
      "\n",
      "[[23241    81    63]\n",
      " [  106   196     6]\n",
      " [  146     6   171]]\n",
      "\n",
      "=== Temporal Tolerance Evaluation ===\n",
      "\n",
      "Temporal event evaluation (+/- 2 frames)\n",
      "----------------------------------------------------\n",
      "Event        | Precision  | Recall     | F1-Score  \n",
      "----------------------------------------------------\n",
      "Bounce       |      0.968 |      0.740 |      0.839\n",
      "Hit          |      0.925 |      0.607 |      0.733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/rf_model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    class_weight=\"balanced\",   # to help with class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Time-aware CV to preserve order of the frames and a gap to avoid data leakage\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [40, 50,\n",
    "    #  60\n",
    "    ],\n",
    "    \"min_samples_split\": [6, 7],\n",
    "    \"min_samples_leaf\": [2,3,\n",
    "    # 4\n",
    "    ],\n",
    "    \"max_features\": [\"sqrt\",\n",
    "    #  \"log2\", None\n",
    "    ]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1_macro\", # Each class’s F1 contributes equally, to help with class imbalance\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "temporal_event_eval(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=True\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"model/rf_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fcae3",
   "metadata": {},
   "source": [
    "## 1.2 Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1) Time-aware CV with a small gap to avoid centered-window bleed\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "# 2) Balanced RF (undersampling per tree)\n",
    "rf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3) Lean grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"precision\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "temporal_event_eval(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=True\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"unused_models/rfus_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd92fff",
   "metadata": {},
   "source": [
    "## 2.1 XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfe838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unused_models/xgb_model.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights per class\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_labeled\n",
    ")\n",
    "\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights[y_train_labeled > 0] *= 5\n",
    "\n",
    "f05_scorer = make_scorer(\n",
    "    fbeta_score,\n",
    "    beta=0.5,\n",
    "    average=\"macro\"\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(classes),\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_estimators=400,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=SMOOTH_WINDOW // 2 + 1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 6],\n",
    "    \"learning_rate\": [0.03, 0.07],\n",
    "    \"subsample\": [0.7, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 1.0],\n",
    "    \"min_child_weight\": [1, 5],\n",
    "    \"gamma\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=f05_scorer,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(\n",
    "    X_train,\n",
    "    y_train_labeled,\n",
    "    sample_weight=sample_weights\n",
    ")\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "best_xgb = grid.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "temporal_event_eval(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"unused_models/xgb_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2dd7ec",
   "metadata": {},
   "source": [
    "## 2.2 XG BOOST with Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b75d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unused_models/xgbus_model.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "pipe = ImbPipeline(steps=[\n",
    "    (\"rus\", RandomUnderSampler(random_state=42)),  # undersampling de la majority class\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "param_grid = {\n",
    "    \"xgb__max_depth\": [3, 6],\n",
    "    \"xgb__learning_rate\": [0.05, 0.1],\n",
    "    \"xgb__subsample\": [0.7, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.7, 1.0],\n",
    "    \"xgb__min_child_weight\": [1, 5],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train_labeled)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "best_pipe = grid.best_estimator_\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "temporal_event_eval(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"unused_models/xgbus_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0f77f",
   "metadata": {},
   "source": [
    "## 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a587dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m618/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4818 — val_f1_macro: 0.3364\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2.0180 - val_loss: 1.1712 - learning_rate: 0.0010 - val_f1_macro: 0.3364\n",
      "Epoch 2/30\n",
      "\u001b[1m627/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3790 — val_f1_macro: 0.3625\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.3700 - val_loss: 0.9520 - learning_rate: 0.0010 - val_f1_macro: 0.3625\n",
      "Epoch 3/30\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2056 — val_f1_macro: 0.3529\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.1708 - val_loss: 1.0100 - learning_rate: 0.0010 - val_f1_macro: 0.3529\n",
      "Epoch 4/30\n",
      "\u001b[1m615/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0321 — val_f1_macro: 0.3531\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.1089 - val_loss: 0.9037 - learning_rate: 0.0010 - val_f1_macro: 0.3531\n",
      "Epoch 5/30\n",
      "\u001b[1m617/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9849 — val_f1_macro: 0.3715\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.0081 - val_loss: 0.8454 - learning_rate: 0.0010 - val_f1_macro: 0.3715\n",
      "Epoch 6/30\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9963 — val_f1_macro: 0.3891\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.9668 - val_loss: 0.7698 - learning_rate: 0.0010 - val_f1_macro: 0.3891\n",
      "Epoch 7/30\n",
      "\u001b[1m622/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8808 — val_f1_macro: 0.3835\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.9310 - val_loss: 0.7994 - learning_rate: 0.0010 - val_f1_macro: 0.3835\n",
      "Epoch 8/30\n",
      "\u001b[1m612/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9282 — val_f1_macro: 0.3803\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.9384 - val_loss: 0.7879 - learning_rate: 0.0010 - val_f1_macro: 0.3803\n",
      "Epoch 9/30\n",
      "\u001b[1m612/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8749 — val_f1_macro: 0.3904\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.8517 - val_loss: 0.7375 - learning_rate: 0.0010 - val_f1_macro: 0.3904\n",
      "Epoch 10/30\n",
      "\u001b[1m623/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8813 — val_f1_macro: 0.4261\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.8552 - val_loss: 0.5983 - learning_rate: 0.0010 - val_f1_macro: 0.4261\n",
      "Epoch 11/30\n",
      "\u001b[1m618/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9371 — val_f1_macro: 0.4041\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.8320 - val_loss: 0.6949 - learning_rate: 0.0010 - val_f1_macro: 0.4041\n",
      "Epoch 12/30\n",
      "\u001b[1m625/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8592 — val_f1_macro: 0.3809\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.8263 - val_loss: 0.7822 - learning_rate: 0.0010 - val_f1_macro: 0.3809\n",
      "Epoch 13/30\n",
      "\u001b[1m622/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7280 — val_f1_macro: 0.3810\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.7624 - val_loss: 0.7908 - learning_rate: 0.0010 - val_f1_macro: 0.3810\n",
      "Epoch 14/30\n",
      "\u001b[1m617/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7337 — val_f1_macro: 0.4012\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.7078 - val_loss: 0.7285 - learning_rate: 5.0000e-04 - val_f1_macro: 0.4012\n",
      "Epoch 15/30\n",
      "\u001b[1m613/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6730 — val_f1_macro: 0.3966\n",
      "Early stopping on macro F1 (patience=5). Restoring best weights.\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.7029 - val_loss: 0.7483 - learning_rate: 5.0000e-04 - val_f1_macro: 0.3966\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "=== Standard Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         air       1.00      0.82      0.90     23385\n",
      "      bounce       0.13      0.94      0.23       308\n",
      "         hit       0.11      0.93      0.20       323\n",
      "\n",
      "    accuracy                           0.82     24016\n",
      "   macro avg       0.41      0.90      0.44     24016\n",
      "weighted avg       0.98      0.82      0.88     24016\n",
      "\n",
      "[[19099  1942  2344]\n",
      " [    4   289    15]\n",
      " [    9    13   301]]\n",
      "\n",
      "=== Temporal Tolerance Evaluation ===\n",
      "\n",
      "Temporal event evaluation (+/- 2 frames)\n",
      "----------------------------------------------------\n",
      "Event        | Precision  | Recall     | F1-Score  \n",
      "----------------------------------------------------\n",
      "Class 1      |      0.541 |      0.974 |      0.695\n",
      "Class 2      |      0.380 |      0.985 |      0.548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['unused_models/mlp_model.joblib']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== Make a small validation split from the tail of train (chronological) ======\n",
    "val_ratio = 0.1\n",
    "split_idx = int((1.0 - val_ratio) * len(X_train_scaled))\n",
    "X_train_mlp, X_val_mlp = X_train_scaled[:split_idx], X_train_scaled[split_idx:]\n",
    "y_train_mlp, y_val_mlp = y_train_labeled[:split_idx], y_train_labeled[split_idx:]\n",
    "\n",
    "\n",
    "# --- Your model builder ---\n",
    "def build_mlp(input_dim, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[]  \n",
    "    )\n",
    "    return model\n",
    "\n",
    "mlp = build_mlp(X_train_mlp.shape[1], num_classes)\n",
    "\n",
    "# --- Macro F1 callback ---\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val, patience=5):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = -np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        f1_macro = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs = logs or {}\n",
    "        logs[\"val_f1_macro\"] = f1_macro\n",
    "        print(f\" — val_f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "        if f1_macro > self.best_f1:\n",
    "            self.best_f1 = f1_macro\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping on macro F1 (patience={self.patience}). Restoring best weights.\")\n",
    "                self.model.stop_training = True\n",
    "                if self.best_weights is not None:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "macro_f1_cb = MacroF1Callback(X_val_mlp, y_val_mlp, patience=5)\n",
    "\n",
    "# --- Other callbacks for stability ---\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    macro_f1_cb,\n",
    "]\n",
    "\n",
    "# Weights per class\n",
    "sample_weights_mlp = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_mlp\n",
    ")\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights_mlp[y_train_mlp > 0] *= 5\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train_mlp, y_train_mlp,\n",
    "    validation_data=(X_val_mlp, y_val_mlp),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    sample_weight=sample_weights_mlp,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# ====== Evaluate on test ======\n",
    "y_proba = mlp.predict(X_test_scaled, batch_size=256)\n",
    "y_pred  = y_proba.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "temporal_event_eval(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(mlp, \"unused_models/mlp_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb986f7",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52b784b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.0158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.2685\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - loss: 1.5060 - val_loss: 1.4534 - learning_rate: 0.0010 - val_f1_macro: 0.2685\n",
      "Epoch 2/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.0111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.3177\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.9804 - val_loss: 1.2828 - learning_rate: 0.0010 - val_f1_macro: 0.3177\n",
      "Epoch 3/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.3441\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.8453 - val_loss: 1.0859 - learning_rate: 0.0010 - val_f1_macro: 0.3441\n",
      "Epoch 4/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.3604\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.7709 - val_loss: 1.0140 - learning_rate: 0.0010 - val_f1_macro: 0.3604\n",
      "Epoch 5/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.3735\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.7224 - val_loss: 0.8138 - learning_rate: 0.0010 - val_f1_macro: 0.3735\n",
      "Epoch 6/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.3781\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.6621 - val_loss: 1.2223 - learning_rate: 0.0010 - val_f1_macro: 0.3781\n",
      "Epoch 7/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4024\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.6223 - val_loss: 0.7052 - learning_rate: 0.0010 - val_f1_macro: 0.4024\n",
      "Epoch 8/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4271\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.5758 - val_loss: 0.5894 - learning_rate: 0.0010 - val_f1_macro: 0.4271\n",
      "Epoch 9/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.3559\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.5568 - val_loss: 1.0072 - learning_rate: 0.0010 - val_f1_macro: 0.3559\n",
      "Epoch 10/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5774"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4083\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.5222 - val_loss: 0.7479 - learning_rate: 0.0010 - val_f1_macro: 0.4083\n",
      "Epoch 11/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4212\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.5353 - val_loss: 0.7099 - learning_rate: 0.0010 - val_f1_macro: 0.4212\n",
      "Epoch 12/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4106\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.4586 - val_loss: 0.7648 - learning_rate: 0.0010 - val_f1_macro: 0.4106\n",
      "Epoch 13/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4945"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4764\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.4942 - val_loss: 0.4678 - learning_rate: 0.0010 - val_f1_macro: 0.4764\n",
      "Epoch 14/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4754\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 0.4216 - val_loss: 0.5069 - learning_rate: 0.0010 - val_f1_macro: 0.4754\n",
      "Epoch 15/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4167\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.4223 - val_loss: 0.6819 - learning_rate: 0.0010 - val_f1_macro: 0.4167\n",
      "Epoch 16/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4673\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.4187 - val_loss: 0.5202 - learning_rate: 0.0010 - val_f1_macro: 0.4673\n",
      "Epoch 17/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3739"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4624\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - loss: 0.3560 - val_loss: 0.5306 - learning_rate: 0.0010 - val_f1_macro: 0.4624\n",
      "Epoch 18/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4816\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.3526 - val_loss: 0.4929 - learning_rate: 0.0010 - val_f1_macro: 0.4816\n",
      "Epoch 19/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4849\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.3382 - val_loss: 0.4924 - learning_rate: 0.0010 - val_f1_macro: 0.4849\n",
      "Epoch 20/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5038\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.3216 - val_loss: 0.4516 - learning_rate: 0.0010 - val_f1_macro: 0.5038\n",
      "Epoch 21/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4675\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.2780 - val_loss: 0.5189 - learning_rate: 0.0010 - val_f1_macro: 0.4675\n",
      "Epoch 22/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4265\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.2802 - val_loss: 0.7673 - learning_rate: 0.0010 - val_f1_macro: 0.4265\n",
      "Epoch 23/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4794\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.2670 - val_loss: 0.5363 - learning_rate: 0.0010 - val_f1_macro: 0.4794\n",
      "Epoch 24/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5214\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - loss: 0.2127 - val_loss: 0.4399 - learning_rate: 0.0010 - val_f1_macro: 0.5214\n",
      "Epoch 25/40\n",
      "\u001b[1m628/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1817"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5261\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - loss: 0.1884 - val_loss: 0.3847 - learning_rate: 0.0010 - val_f1_macro: 0.5261\n",
      "Epoch 26/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5338\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - loss: 0.1868 - val_loss: 0.3886 - learning_rate: 0.0010 - val_f1_macro: 0.5338\n",
      "Epoch 27/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4769\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - loss: 0.2424 - val_loss: 0.5325 - learning_rate: 0.0010 - val_f1_macro: 0.4769\n",
      "Epoch 28/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5353\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - loss: 0.2149 - val_loss: 0.3641 - learning_rate: 0.0010 - val_f1_macro: 0.5353\n",
      "Epoch 29/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1995"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4934\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - loss: 0.2105 - val_loss: 0.5214 - learning_rate: 0.0010 - val_f1_macro: 0.4934\n",
      "Epoch 30/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5561\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - loss: 0.1905 - val_loss: 0.3157 - learning_rate: 0.0010 - val_f1_macro: 0.5561\n",
      "Epoch 31/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4752\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - loss: 0.2039 - val_loss: 0.5789 - learning_rate: 0.0010 - val_f1_macro: 0.4752\n",
      "Epoch 32/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5418\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - loss: 0.2089 - val_loss: 0.3349 - learning_rate: 0.0010 - val_f1_macro: 0.5418\n",
      "Epoch 33/40\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.4955\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - loss: 0.1784 - val_loss: 0.5340 - learning_rate: 0.0010 - val_f1_macro: 0.4955\n",
      "Epoch 34/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5402\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - loss: 0.1674 - val_loss: 0.3564 - learning_rate: 0.0010 - val_f1_macro: 0.5402\n",
      "Epoch 35/40\n",
      "\u001b[1m629/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5163\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - loss: 0.1886 - val_loss: 0.4293 - learning_rate: 0.0010 - val_f1_macro: 0.5163\n",
      "Epoch 36/40\n",
      "\u001b[1m630/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasab/miniforge3/envs/hit_bounce_env/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:171: UserWarning: Learning rate reduction is conditioned on metric `val_f1_macro` which is not available. Available metrics are: loss,val_loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — val_f1_macro: 0.5550\n",
      "Early stopping on macro F1 (patience=6). Restoring best weights.\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - loss: 0.1822 - val_loss: 0.3470 - learning_rate: 0.0010 - val_f1_macro: 0.5550\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "\n",
      "=== Standard Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         air       1.00      0.93      0.97     23379\n",
      "      bounce       0.25      0.95      0.39       308\n",
      "         hit       0.31      0.93      0.47       323\n",
      "\n",
      "    accuracy                           0.93     24010\n",
      "   macro avg       0.52      0.94      0.61     24010\n",
      "weighted avg       0.98      0.93      0.95     24010\n",
      "\n",
      "[[21828   898   653]\n",
      " [    9   294     5]\n",
      " [   19     5   299]]\n",
      "\n",
      "=== Temporal Tolerance Evaluation ===\n",
      "\n",
      "Temporal event evaluation (+/- 2 frames)\n",
      "----------------------------------------------------\n",
      "Event        | Precision  | Recall     | F1-Score  \n",
      "----------------------------------------------------\n",
      "Class 1      |      0.707 |      0.977 |      0.820\n",
      "Class 2      |      0.737 |      0.978 |      0.840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['unused_models/lstm_model.joblib']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Macro F1 callback -----\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val, patience=6):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = -np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val, verbose=0, batch_size=128)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        f1_macro = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs = logs or {}\n",
    "        logs[\"val_f1_macro\"] = f1_macro\n",
    "        print(f\" — val_f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "        if f1_macro > self.best_f1:\n",
    "            self.best_f1 = f1_macro\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping on macro F1 (patience={self.patience}). Restoring best weights.\")\n",
    "                self.model.stop_training = True\n",
    "                if self.best_weights is not None:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "def build_lstm(window_size, feature_dim, num_classes, bidirectional=False):\n",
    "    inputs = keras.Input(shape=(window_size, feature_dim))\n",
    "\n",
    "    x = inputs\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    else:\n",
    "        x = layers.LSTM(64, return_sequences=True)(x)\n",
    "        x = layers.LSTM(64)(x)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[]  \n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ====== Sequences (from your code) ======\n",
    "def make_sequences(X: np.ndarray, y: np.ndarray, window: int = 7, stride: int = 1):\n",
    "    X_seq, y_seq = [], []\n",
    "    for start in range(0, len(X) - window + 1, stride):\n",
    "        end = start + window\n",
    "        X_seq.append(X[start:end])\n",
    "        mid_idx = start + window // 2\n",
    "        y_seq.append(y[mid_idx])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "window = 7\n",
    "stride = 1\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train_deep_scaled, y_train_labeled, window=window, stride=stride)\n",
    "X_test_seq,  y_test_seq  = make_sequences(X_test_deep_scaled,  y_test_labeled,  window=window, stride=stride)\n",
    "\n",
    "num_classes = len(classes)\n",
    "feature_dim = X_train_seq.shape[-1]\n",
    "\n",
    "# Chronological validation split (tail)\n",
    "val_ratio = 0.1\n",
    "split_idx = int((1.0 - val_ratio) * len(X_train_seq))\n",
    "X_train_lstm, X_val_lstm = X_train_seq[:split_idx], X_train_seq[split_idx:]\n",
    "y_train_lstm, y_val_lstm = y_train_seq[:split_idx], y_train_seq[split_idx:]\n",
    "\n",
    "# Weights per class\n",
    "sample_weights_lstm = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_lstm\n",
    ")\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights_lstm[y_train_lstm > 0] *= 5\n",
    "\n",
    "# ====== Build LSTM ======\n",
    "lstm = build_lstm(window, feature_dim, num_classes, bidirectional=True)\n",
    "\n",
    "# Callbacks: LR on val_loss, early stop/restore on val macro F1 via custom callback\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_f1_macro\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    MacroF1Callback(X_val_lstm, y_val_lstm, patience=6),\n",
    "]\n",
    "\n",
    "# ====== Train ======\n",
    "history = lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_val_lstm, y_val_lstm),\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    sample_weight=sample_weights_lstm,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# ====== Evaluate ======\n",
    "y_proba_seq = lstm.predict(X_test_seq, batch_size=128)\n",
    "y_pred_seq  = y_proba_seq.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_seq,\n",
    "    y_pred_seq,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_seq, y_pred_seq))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "temporal_event_eval(\n",
    "    y_test_seq,\n",
    "    y_pred_seq,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(lstm, \"unused_models/lstm_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4735a",
   "metadata": {},
   "source": [
    "### Feature permutaion of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_feature_importance_lstm(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    metric_fn,\n",
    "    n_repeats=3,\n",
    "    batch_size=1024,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    X shape: (samples, timesteps, features)\n",
    "    Returns: importance array of shape (features,)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    # Baseline score\n",
    "    y_proba = model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    y_pred = y_proba.argmax(axis=1)\n",
    "    baseline_score = metric_fn(y, y_pred)\n",
    "\n",
    "    n_features = X.shape[-1]\n",
    "    importances = np.zeros(n_features)\n",
    "\n",
    "    for f in range(n_features):\n",
    "        scores = []\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            X_perm = X.copy()\n",
    "\n",
    "            # Shuffle feature f **across samples**, keeping time structure\n",
    "            perm_idx = rng.permutation(X_perm.shape[0])\n",
    "            X_perm[:, :, f] = X_perm[perm_idx, :, f]\n",
    "\n",
    "            y_proba_perm = model.predict(X_perm, batch_size=batch_size, verbose=0)\n",
    "            y_pred_perm = y_proba_perm.argmax(axis=1)\n",
    "\n",
    "            score = metric_fn(y, y_pred_perm)\n",
    "            scores.append(score)\n",
    "\n",
    "        importances[f] = baseline_score - np.mean(scores)\n",
    "\n",
    "    return importances, baseline_score\n",
    "\n",
    "\n",
    "# ====== Feature Importance (Permutation) ======\n",
    "feature_importance, baseline_f1 = permutation_feature_importance_lstm(\n",
    "    model=lstm,\n",
    "    X=X_val_lstm,                # use validation set\n",
    "    y=y_val_lstm,\n",
    "    metric_fn=lambda y_true, y_pred: f1_score(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    ),\n",
    "    n_repeats=5,\n",
    ")\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"importance\": feature_importance,\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Baseline macro F1:\", baseline_f1)\n",
    "print(feature_importance_df)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    feature_importance_df[\"feature\"],\n",
    "    feature_importance_df[\"importance\"]\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Permutation Feature Importance (LSTM)\")\n",
    "plt.xlabel(\"Decrease in Macro F1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
