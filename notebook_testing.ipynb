{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, f1_score, make_scorer, fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98930c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory\n",
    "path_dir = Path.cwd()\n",
    "\n",
    "# Folder with the JSON files\n",
    "json_dir = path_dir / \"per_point_v2\"\n",
    "\n",
    "# Prepare an empty DataFrame with the expected columns and an index name\n",
    "df = pd.DataFrame(columns=[\"x\", \"y\", \"visible\", \"action\"])\n",
    "df.index.name = \"image_frame\"\n",
    "\n",
    "frames_df = []\n",
    "\n",
    "for json_path in json_dir.glob(\"*.json\"):\n",
    "    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        ball_data = json.load(f)  # expected: dict keyed by image_frame\n",
    "\n",
    "    # Build a DataFrame from the JSON dict, then transpose:\n",
    "    file_df = pd.DataFrame(ball_data).T\n",
    "    file_df.index.name = \"image_frame\"\n",
    "\n",
    "    # Ensure column names match the expected schema\n",
    "    file_df = file_df.reindex(columns=[\"x\", \"y\", \"visible\", \"action\"])\n",
    "\n",
    "    frames_df.append(file_df)\n",
    "\n",
    "# Final concatenation\n",
    "df = pd.concat(frames_df, axis=0, ignore_index=False)\n",
    "df.index.name = \"image_frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991dc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(\n",
    "    subset_df: pd.DataFrame,\n",
    "    smooth_window: int = 7,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature builder for ball hit / bounce detection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Numeric positions and index\n",
    "    # ------------------------------------------------------------------\n",
    "    subset = subset_df.copy()\n",
    "    subset.index = pd.to_numeric(subset.index, errors=\"coerce\")\n",
    "    subset = subset.sort_index()\n",
    "    subset[\"x_i\"] = pd.to_numeric(subset[\"x\"], errors=\"coerce\")\n",
    "    subset[\"y_i\"] = pd.to_numeric(subset[\"y\"], errors=\"coerce\")\n",
    "    subset = subset.dropna(subset=[\"x_i\", \"y_i\"])\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw positions\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"x_raw\"] = subset[\"x_i\"]\n",
    "    subset[\"y_raw\"] = subset[\"y_i\"]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Centered smoothing on positions\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Centered rolling mean reduces high-frequency measurement noise\n",
    "    # without eliminating physical discontinuities (hits / bounces).\n",
    "    subset[\"x_smooth\"] = (\n",
    "        subset[\"x_raw\"]\n",
    "        .rolling(smooth_window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "    subset[\"y_smooth\"] = (\n",
    "        subset[\"y_raw\"]\n",
    "        .rolling(smooth_window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Time step (central)\n",
    "    # ------------------------------------------------------------------\n",
    "    t = subset.index.to_series()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Smoothed derivatives (stable kinematics)\n",
    "    # ------------------------------------------------------------------\n",
    "    x_smooth = subset[\"x_smooth\"].to_numpy()\n",
    "    y_smooth = subset[\"y_smooth\"].to_numpy()\n",
    "\n",
    "    vx = np.gradient(x_smooth, t)\n",
    "    vy = np.gradient(y_smooth, t)\n",
    "\n",
    "    ax = np.gradient(vx, t)\n",
    "    ay = np.gradient(vy, t)\n",
    "\n",
    "    jx = np.gradient(ax, t)\n",
    "    jy = np.gradient(ay, t)\n",
    "\n",
    "    subset[\"vx\"] = vx\n",
    "    subset[\"vy\"] = vy\n",
    "    subset[\"ax\"] = ax\n",
    "    subset[\"ay\"] = ay\n",
    "    subset[\"jx\"] = jx\n",
    "    subset[\"jy\"] = jy\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw derivatives (impulse-sensitive)\n",
    "    # ------------------------------------------------------------------\n",
    "    x_raw = subset[\"x_raw\"].to_numpy()\n",
    "    y_raw = subset[\"y_raw\"].to_numpy()\n",
    "\n",
    "    vx_raw = np.gradient(x_raw, t)\n",
    "    vy_raw = np.gradient(y_raw, t)\n",
    "\n",
    "    ax_raw = np.gradient(vx_raw, t)\n",
    "    ay_raw = np.gradient(vy_raw, t)\n",
    "\n",
    "    jx_raw = np.gradient(ax_raw, t)\n",
    "    jy_raw = np.gradient(ay_raw, t)\n",
    "\n",
    "    subset[\"vx_raw\"] = vx_raw\n",
    "    subset[\"vy_raw\"] = vy_raw\n",
    "    subset[\"ax_raw\"] = ax_raw\n",
    "    subset[\"ay_raw\"] = ay_raw\n",
    "    subset[\"jx_raw\"] = jx_raw\n",
    "    subset[\"jy_raw\"] = jy_raw\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw derivatubes in absolute\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    subset[\"vx_abs_raw\"] = np.abs(subset[\"vx_raw\"])\n",
    "    subset[\"vy_abs_raw\"] = np.abs(subset[\"vy_raw\"])\n",
    "    subset[\"ax_abs_raw\"] = np.abs(subset[\"ax_raw\"])\n",
    "    subset[\"ay_abs_raw\"] = np.abs(subset[\"ay_raw\"])\n",
    "    subset[\"jx_abs_raw\"] = np.abs(subset[\"jx_raw\"])\n",
    "    subset[\"jy_abs_raw\"] = np.abs(subset[\"jy_raw\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Magnitudes (smoothed)\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"v\"] = np.sqrt(subset[\"vx\"]**2 + subset[\"vy\"]**2)\n",
    "    subset[\"a\"] = np.sqrt(subset[\"ax\"]**2 + subset[\"ay\"]**2)\n",
    "    subset[\"jerk\"] = np.sqrt(subset[\"jx\"]**2 + subset[\"jy\"]**2)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Log magnitudes : preserves order and compresses large values\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"log_v\"] = np.log1p(subset[\"v\"])    \n",
    "    subset[\"log_a\"] = np.log1p(subset[\"a\"])\n",
    "    subset[\"log_j\"] = np.log1p(subset[\"jerk\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Directional features\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"angle\"] = np.arctan2(subset[\"vy\"], subset[\"vx\"])\n",
    "    subset[\"delta_angle\"] = np.gradient(subset[\"angle\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Centered rolling statistics (smoothed)\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"v_mean\"] = subset[\"v\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    subset[\"v_std\"]  = subset[\"v\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    subset[\"a_mean\"] = subset[\"a\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    subset[\"a_std\"]  = subset[\"a\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    subset[\"j_mean\"] = subset[\"jerk\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    subset[\"j_std\"]  = subset[\"jerk\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Motion sign changes\n",
    "    # ------------------------------------------------------------------\n",
    "    subset[\"vx_sign\"] = np.sign(subset[\"vx\"]).fillna(0.0)\n",
    "    subset[\"vx_sign_change\"] = (\n",
    "        subset[\"vx_sign\"].diff().abs() > 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    subset[\"vy_sign\"] = np.sign(subset[\"vy\"]).fillna(0.0)\n",
    "    subset[\"vy_sign_change\"] = (\n",
    "        subset[\"vy_sign\"].diff().abs() > 0\n",
    "    ).astype(int)\n",
    "\n",
    "    return subset\n",
    "\n",
    "# Select features\n",
    "FEATURE_COLS = [\n",
    "    \"delta_angle\",\n",
    "    \"vx_sign_change\", \n",
    "    \"vy_sign_change\",\n",
    "    \"v\", \"a\", \"jerk\",\n",
    "    \"vx\", 'vy', 'ax', 'ay', 'jx', 'jy',\n",
    "    \"v_mean\", \"v_std\",\n",
    "    \"a_mean\", \"a_std\",\n",
    "    \"j_mean\", \"j_std\",\n",
    "    \"log_v\", \"log_a\", \"log_j\",\n",
    "    \"vx_abs_raw\", \"vy_abs_raw\",\n",
    "    \"ax_abs_raw\", \"ay_abs_raw\",\n",
    "    \"jx_abs_raw\", \"jy_abs_raw\",\n",
    "]\n",
    "FEATURE_COLS_DEEP = FEATURE_COLS + [\"x_i\", \"y_i\"]\n",
    "SMOOTH_WINDOW = 7\n",
    "\n",
    "# Selecting df\n",
    "df_copy = df.copy()\n",
    "df_copy.index = pd.to_numeric(df_copy.index, errors=\"coerce\")\n",
    "df_copy = df_copy.sort_index()\n",
    "\n",
    "# Train-Test Split\n",
    "split_point = int(0.8 * len(df_copy))\n",
    "train_df_raw = df_copy.iloc[:split_point]\n",
    "test_df_raw  = df_copy.iloc[split_point:]\n",
    "\n",
    "# DataFrames with processed features\n",
    "train_df = build_features(train_df_raw, smooth_window=SMOOTH_WINDOW)\n",
    "test_df  = build_features(test_df_raw,  smooth_window=SMOOTH_WINDOW)\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "X_test  = test_df[FEATURE_COLS]\n",
    "X_train_deep = train_df[FEATURE_COLS_DEEP]\n",
    "X_test_deep  = test_df[FEATURE_COLS_DEEP]\n",
    "y_train = train_df[\"action\"].to_numpy()\n",
    "y_test  = test_df[\"action\"].to_numpy()\n",
    "\n",
    "# Scaling (fit on train, apply to test)\n",
    "scaler = StandardScaler()\n",
    "scaler_deep = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "X_train_deep_scaled = scaler_deep.fit_transform(X_train_deep)\n",
    "X_test_deep_scaled  = scaler_deep.transform(X_test_deep)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_labeled = le.fit_transform(y_train)\n",
    "y_test_labeled  = le.transform(y_test)\n",
    "classes = le.classes_\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Saving preprocessors\n",
    "preprocessors = {\n",
    "    \"scaler\": scaler,\n",
    "    \"scaler_deep\": scaler_deep,\n",
    "    \"label_encoder\": le\n",
    "}\n",
    "joblib.dump(preprocessors, \"preprocessors.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d32a3e",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_with_and_without_tolerance(y_true, y_pred, tolerance=2, use_labels=True):\n",
    "    \"\"\"\n",
    "    Event-level evaluation for temporal predictions with +/- tolerance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth events (can be numeric or string labels)\n",
    "    y_pred : array-like\n",
    "        Predicted events (same format as y_true)\n",
    "    tolerance : int\n",
    "        Number of frames before/after to consider a prediction correct\n",
    "    use_labels : bool\n",
    "        If True, expects string labels like \"air\", \"bounce\", \"hit\".\n",
    "        If False, expects numeric labels like 0, 1, 2.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if use_labels:\n",
    "        event_classes = [\"bounce\", \"hit\"]\n",
    "    else:\n",
    "        event_classes = [1, 2]\n",
    "\n",
    "    print(f\"\\nTemporal event evaluation (+/- {tolerance} frames)\")\n",
    "    print(\"-\" * 52)\n",
    "    print(f\"{'Event':<12} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10}\")\n",
    "    print(\"-\" * 52)\n",
    "\n",
    "    for event in event_classes:\n",
    "        true_indices = np.where(y_true == event)[0]\n",
    "        pred_indices = np.where(y_pred == event)[0]\n",
    "\n",
    "        # ---------- Recall ----------\n",
    "        matched_true = np.array([np.any(np.abs(pred_indices - t) <= tolerance) for t in true_indices])\n",
    "        recall = matched_true.sum() / len(true_indices) if len(true_indices) > 0 else 0.0\n",
    "\n",
    "        # ---------- Precision ----------\n",
    "        matched_pred = np.array([np.any(np.abs(true_indices - p) <= tolerance) for p in pred_indices])\n",
    "        precision = matched_pred.sum() / len(pred_indices) if len(pred_indices) > 0 else 0.0\n",
    "\n",
    "        # ---------- F1 ----------\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        event_name = str(event).capitalize() if use_labels else f\"Class {event}\"\n",
    "        print(f\"{event_name:<12} | {precision:>10.3f} | {recall:>10.3f} | {f1:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef770f12",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b39cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Estimate heuristic thresholds\n",
    "# ==============================\n",
    "def estimate_heuristic_thresholds(train_df, window=3):\n",
    "    ay_abs = train_df[\"ay_abs_raw\"].values\n",
    "    ay = train_df[\"ay_raw\"].values\n",
    "    vx = train_df[\"vx_raw\"].values\n",
    "    vy = train_df[\"vy_raw\"].values\n",
    "    jerk = train_df[\"jerk\"].values\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"AY_PEAK_MIN\": np.percentile(ay_abs, 70),\n",
    "        \"PROMINENCE\": np.percentile(ay_abs, 85),\n",
    "        \"VX_ENERGY_DELTA\": np.percentile(np.abs(np.diff(vx)), 90),\n",
    "        \"VX_MIN_MOVE\": np.percentile(np.abs(vx), 10),\n",
    "        \"VY_MIN_MOVE\": np.percentile(np.abs(vy), 10),\n",
    "        \"AY_CONCAVE_MAX\": np.percentile(ay, 10),\n",
    "        \"jerk_peak\": np.percentile(jerk, 90),\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Heuristic event detector\n",
    "# ==============================\n",
    "def heuristic_event_detector(features_df, thresholds, refractory=10, window=3):\n",
    "    df = features_df.copy()\n",
    "\n",
    "    # Raw signals\n",
    "    ay_abs = df[\"ay_abs_raw\"].values\n",
    "    ay = df[\"ay_raw\"].values\n",
    "    vx = df[\"vx_raw\"].values\n",
    "    vy = df[\"vy_raw\"].values\n",
    "    jx = df[\"jx_raw\"].values\n",
    "    jy = df[\"jy_raw\"].values\n",
    "    jerk = np.sqrt(jx**2 + jy**2)\n",
    "\n",
    "    # Peak detection (candidates)\n",
    "    peaks, _ = find_peaks(\n",
    "        ay_abs,\n",
    "        height=thresholds[\"AY_PEAK_MIN\"],\n",
    "        prominence=thresholds[\"PROMINENCE\"],\n",
    "        distance=3\n",
    "    )\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # Candidate evaluation\n",
    "    for i in peaks:\n",
    "        if i < window or i + window >= len(df):\n",
    "            continue\n",
    "\n",
    "        # Pre / post impact states\n",
    "        vx_pre, vx_post = vx[i - window], vx[i + window]\n",
    "        vy_pre, vy_post = vy[i - window], vy[i + window]\n",
    "        ay_val = ay[i]\n",
    "        jerk_val = jerk[i]\n",
    "\n",
    "        # Core physics features\n",
    "        delta_vx = abs(vx_post) - abs(vx_pre)\n",
    "        vx_flip = vx_pre * vx_post < 0\n",
    "        vy_flip = vy_pre * vy_post < 0\n",
    "        vy_loss = abs(vy_post) / (abs(vy_pre) + 1e-6)\n",
    "        angle_pre = np.arctan2(vy_pre, vx_pre)\n",
    "        angle_post = np.arctan2(vy_post, vx_post)\n",
    "        angle_change = abs(angle_post - angle_pre)\n",
    "\n",
    "        # Scoring (soft voting)\n",
    "        hit_score = 0.0\n",
    "        bounce_score = 0.0\n",
    "\n",
    "        # HIT indicators\n",
    "        hit_score += 2.0 if vx_flip else 0.0\n",
    "        hit_score += 1.5 if delta_vx > thresholds[\"VX_ENERGY_DELTA\"] else 0.0\n",
    "        hit_score += 1.0 if vy_loss > 1.1 else 0.0\n",
    "        hit_score += 1.0 if jerk_val > thresholds[\"jerk_peak\"] else 0.0\n",
    "        hit_score += 1.0 if angle_change > np.pi / 4 else 0.0\n",
    "\n",
    "        # BOUNCE indicators\n",
    "        bounce_score += 2.0 if ay_val < thresholds[\"AY_CONCAVE_MAX\"] else 0.0\n",
    "        bounce_score += 1.5 if vy_flip else 0.0\n",
    "        bounce_score += 1.0 if vy_loss < 0.8 else 0.0\n",
    "\n",
    "        # Decision\n",
    "        pred = None\n",
    "        priority = 0\n",
    "        if hit_score >= bounce_score and hit_score >= 2.5:\n",
    "            pred, priority = \"hit\", 2\n",
    "        elif bounce_score > hit_score and bounce_score >= 2.0:\n",
    "            pred, priority = \"bounce\", 1\n",
    "\n",
    "        # Combined strength score (for NMS)\n",
    "        strength = abs(ay_val) + jerk_val + abs(delta_vx) + angle_change\n",
    "\n",
    "        if pred is not None:\n",
    "            candidates.append((df.index[i], pred, strength, priority))\n",
    "\n",
    "    # Temporal NMS (refractory)\n",
    "    final = {}\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    for frame, label, score, _ in candidates:\n",
    "        if not final:\n",
    "            final[frame] = (label, score)\n",
    "            continue\n",
    "        last_frame = list(final.keys())[-1]\n",
    "        if frame - last_frame >= refractory:\n",
    "            final[frame] = (label, score)\n",
    "        else:\n",
    "            if score > final[last_frame][1]:\n",
    "                final[last_frame] = (label, score)\n",
    "\n",
    "    return {k: v[0] for k, v in final.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "68645512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temporal event evaluation (+/- 2 frames)\n",
      "----------------------------------------------------\n",
      "Event        | Precision  | Recall     | F1-Score  \n",
      "----------------------------------------------------\n",
      "Bounce       |      0.490 |      0.724 |      0.585\n",
      "Hit          |      0.604 |      0.520 |      0.559\n"
     ]
    }
   ],
   "source": [
    "# Evluation \n",
    "thresh = estimate_heuristic_thresholds(train_df)\n",
    "y_pred = heuristic_event_detector(test_df, thresholds=thresh)\n",
    "y_pred_array = np.array([\"air\"] * len(y_test), dtype=object)\n",
    "for frame, label in y_pred.items():\n",
    "    if frame in test_df.index:\n",
    "        idx = test_df.index.get_loc(frame)\n",
    "        y_pred_array[idx] = label\n",
    "\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_array,\n",
    "    use_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a4870",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938a69a",
   "metadata": {},
   "source": [
    "## 1.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d65dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    class_weight=\"balanced\",   # to help with class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Time-aware CV to preserve order of the frames and a gap to avoid data leakage\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [40, 50,\n",
    "    #  60\n",
    "    ],\n",
    "    \"min_samples_split\": [6, 7],\n",
    "    \"min_samples_leaf\": [2,3,\n",
    "    # 4\n",
    "    ],\n",
    "    \"max_features\": [\"sqrt\",\n",
    "    #  \"log2\", None\n",
    "    ]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1_macro\", # Each class’s F1 contributes equally, to help with class imbalance\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=True\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"model/rf_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fcae3",
   "metadata": {},
   "source": [
    "## 1.2 Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1) Time-aware CV with a small gap to avoid centered-window bleed\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "# 2) Balanced RF (undersampling per tree)\n",
    "rf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3) Lean grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"precision\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=True\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"unused_models/rfus_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd92fff",
   "metadata": {},
   "source": [
    "## 2.1 XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfe838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights per class\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_labeled\n",
    ")\n",
    "\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights[y_train_labeled > 0] *= 5\n",
    "\n",
    "f05_scorer = make_scorer(\n",
    "    fbeta_score,\n",
    "    beta=0.5,\n",
    "    average=\"macro\"\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(classes),\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_estimators=400,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=SMOOTH_WINDOW // 2 + 1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 6],\n",
    "    \"learning_rate\": [0.03, 0.07],\n",
    "    \"subsample\": [0.7, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 1.0],\n",
    "    \"min_child_weight\": [1, 5],\n",
    "    \"gamma\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=f05_scorer,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(\n",
    "    X_train,\n",
    "    y_train_labeled,\n",
    "    sample_weight=sample_weights\n",
    ")\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "best_xgb = grid.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"unused_models/xgb_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2dd7ec",
   "metadata": {},
   "source": [
    "## 2.2 XG BOOST with Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b75d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "pipe = ImbPipeline(steps=[\n",
    "    (\"rus\", RandomUnderSampler(random_state=42)),  # undersampling de la majority class\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "param_grid = {\n",
    "    \"xgb__max_depth\": [3, 6],\n",
    "    \"xgb__learning_rate\": [0.05, 0.1],\n",
    "    \"xgb__subsample\": [0.7, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.7, 1.0],\n",
    "    \"xgb__min_child_weight\": [1, 5],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train_labeled)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "best_pipe = grid.best_estimator_\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(grid, \"unused_models/xgbus_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0f77f",
   "metadata": {},
   "source": [
    "## 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a587dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Make a small validation split from the tail of train (chronological) ======\n",
    "val_ratio = 0.1\n",
    "split_idx = int((1.0 - val_ratio) * len(X_train_scaled))\n",
    "X_train_mlp, X_val_mlp = X_train_scaled[:split_idx], X_train_scaled[split_idx:]\n",
    "y_train_mlp, y_val_mlp = y_train_labeled[:split_idx], y_train_labeled[split_idx:]\n",
    "\n",
    "\n",
    "# --- Your model builder ---\n",
    "def build_mlp(input_dim, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[]  \n",
    "    )\n",
    "    return model\n",
    "\n",
    "mlp = build_mlp(X_train_mlp.shape[1], num_classes)\n",
    "\n",
    "# --- Macro F1 callback ---\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val, patience=5):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = -np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        f1_macro = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs = logs or {}\n",
    "        logs[\"val_f1_macro\"] = f1_macro\n",
    "        print(f\" — val_f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "        if f1_macro > self.best_f1:\n",
    "            self.best_f1 = f1_macro\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping on macro F1 (patience={self.patience}). Restoring best weights.\")\n",
    "                self.model.stop_training = True\n",
    "                if self.best_weights is not None:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "macro_f1_cb = MacroF1Callback(X_val_mlp, y_val_mlp, patience=5)\n",
    "\n",
    "# --- Other callbacks for stability ---\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    macro_f1_cb,\n",
    "]\n",
    "\n",
    "# Weights per class\n",
    "sample_weights_mlp = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_mlp\n",
    ")\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights_mlp[y_train_mlp > 0] *= 5\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train_mlp, y_train_mlp,\n",
    "    validation_data=(X_val_mlp, y_val_mlp),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    sample_weight=sample_weights_mlp,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# ====== Evaluate on test ======\n",
    "y_proba = mlp.predict(X_test_scaled, batch_size=256)\n",
    "y_pred  = y_proba.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(mlp, \"unused_models/mlp_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb986f7",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b784b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Macro F1 callback -----\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val, patience=6):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = -np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val, verbose=0, batch_size=128)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        f1_macro = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs = logs or {}\n",
    "        logs[\"val_f1_macro\"] = f1_macro\n",
    "        print(f\" — val_f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "        if f1_macro > self.best_f1:\n",
    "            self.best_f1 = f1_macro\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping on macro F1 (patience={self.patience}). Restoring best weights.\")\n",
    "                self.model.stop_training = True\n",
    "                if self.best_weights is not None:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "def build_lstm(window_size, feature_dim, num_classes, bidirectional=False):\n",
    "    inputs = keras.Input(shape=(window_size, feature_dim))\n",
    "\n",
    "    x = inputs\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    else:\n",
    "        x = layers.LSTM(64, return_sequences=True)(x)\n",
    "        x = layers.LSTM(64)(x)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[]  \n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ====== Sequences (from your code) ======\n",
    "def make_sequences(X: np.ndarray, y: np.ndarray, window: int = 7, stride: int = 1):\n",
    "    X_seq, y_seq = [], []\n",
    "    for start in range(0, len(X) - window + 1, stride):\n",
    "        end = start + window\n",
    "        X_seq.append(X[start:end])\n",
    "        mid_idx = start + window // 2\n",
    "        y_seq.append(y[mid_idx])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "window = 7\n",
    "stride = 1\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train_deep_scaled, y_train_labeled, window=window, stride=stride)\n",
    "X_test_seq,  y_test_seq  = make_sequences(X_test_deep_scaled,  y_test_labeled,  window=window, stride=stride)\n",
    "\n",
    "num_classes = len(classes)\n",
    "feature_dim = X_train_seq.shape[-1]\n",
    "\n",
    "# Chronological validation split (tail)\n",
    "val_ratio = 0.1\n",
    "split_idx = int((1.0 - val_ratio) * len(X_train_seq))\n",
    "X_train_lstm, X_val_lstm = X_train_seq[:split_idx], X_train_seq[split_idx:]\n",
    "y_train_lstm, y_val_lstm = y_train_seq[:split_idx], y_train_seq[split_idx:]\n",
    "\n",
    "# Weights per class\n",
    "sample_weights_lstm = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_lstm\n",
    ")\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights_lstm[y_train_lstm > 0] *= 5\n",
    "\n",
    "# ====== Build LSTM ======\n",
    "lstm = build_lstm(window, feature_dim, num_classes, bidirectional=True)\n",
    "\n",
    "# Callbacks: LR on val_loss, early stop/restore on val macro F1 via custom callback\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_f1_macro\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    MacroF1Callback(X_val_lstm, y_val_lstm, patience=6),\n",
    "]\n",
    "\n",
    "# ====== Train ======\n",
    "history = lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_val_lstm, y_val_lstm),\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    sample_weight=sample_weights_lstm,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# ====== Evaluate ======\n",
    "y_proba_seq = lstm.predict(X_test_seq, batch_size=128)\n",
    "y_pred_seq  = y_proba_seq.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Standard Evaluation ===\")\n",
    "print(classification_report(\n",
    "    y_test_seq,\n",
    "    y_pred_seq,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_seq, y_pred_seq))\n",
    "\n",
    "print(\"\\n=== Temporal Tolerance Evaluation ===\")\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_seq,\n",
    "    y_pred_seq,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(lstm, \"unused_models/lstm_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4735a",
   "metadata": {},
   "source": [
    "### Feature permutaion of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_feature_importance_lstm(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    metric_fn,\n",
    "    n_repeats=3,\n",
    "    batch_size=1024,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    X shape: (samples, timesteps, features)\n",
    "    Returns: importance array of shape (features,)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    # Baseline score\n",
    "    y_proba = model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    y_pred = y_proba.argmax(axis=1)\n",
    "    baseline_score = metric_fn(y, y_pred)\n",
    "\n",
    "    n_features = X.shape[-1]\n",
    "    importances = np.zeros(n_features)\n",
    "\n",
    "    for f in range(n_features):\n",
    "        scores = []\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            X_perm = X.copy()\n",
    "\n",
    "            # Shuffle feature f **across samples**, keeping time structure\n",
    "            perm_idx = rng.permutation(X_perm.shape[0])\n",
    "            X_perm[:, :, f] = X_perm[perm_idx, :, f]\n",
    "\n",
    "            y_proba_perm = model.predict(X_perm, batch_size=batch_size, verbose=0)\n",
    "            y_pred_perm = y_proba_perm.argmax(axis=1)\n",
    "\n",
    "            score = metric_fn(y, y_pred_perm)\n",
    "            scores.append(score)\n",
    "\n",
    "        importances[f] = baseline_score - np.mean(scores)\n",
    "\n",
    "    return importances, baseline_score\n",
    "\n",
    "\n",
    "# ====== Feature Importance (Permutation) ======\n",
    "feature_importance, baseline_f1 = permutation_feature_importance_lstm(\n",
    "    model=lstm,\n",
    "    X=X_val_lstm,                # use validation set\n",
    "    y=y_val_lstm,\n",
    "    metric_fn=lambda y_true, y_pred: f1_score(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    ),\n",
    "    n_repeats=5,\n",
    ")\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"importance\": feature_importance,\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Baseline macro F1:\", baseline_f1)\n",
    "print(feature_importance_df)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    feature_importance_df[\"feature\"],\n",
    "    feature_importance_df[\"importance\"]\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Permutation Feature Importance (LSTM)\")\n",
    "plt.xlabel(\"Decrease in Macro F1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hit_bounce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
