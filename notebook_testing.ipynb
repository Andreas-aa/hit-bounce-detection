{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer, fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98930c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory\n",
    "path_dir = Path.cwd()\n",
    "\n",
    "# Folder with the JSON files\n",
    "json_dir = path_dir / \"per_point_v2\"\n",
    "\n",
    "# Prepare an empty DataFrame with the expected columns and an index name\n",
    "df = pd.DataFrame(columns=[\"x\", \"y\", \"visible\", \"action\"])\n",
    "df.index.name = \"image_frame\"\n",
    "\n",
    "frames_df = []\n",
    "\n",
    "for json_path in json_dir.glob(\"*.json\"):\n",
    "    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        ball_data = json.load(f)  # expected: dict keyed by image_frame\n",
    "\n",
    "    # Build a DataFrame from the JSON dict, then transpose:\n",
    "    file_df = pd.DataFrame(ball_data).T\n",
    "    file_df.index.name = \"image_frame\"\n",
    "\n",
    "    # Ensure column names match the expected schema\n",
    "    file_df = file_df.reindex(columns=[\"x\", \"y\", \"visible\", \"action\"])\n",
    "\n",
    "    frames_df.append(file_df)\n",
    "\n",
    "# Final concatenation\n",
    "df = pd.concat(frames_df, axis=0, ignore_index=False)\n",
    "df.index.name = \"image_frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991dc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(\n",
    "    df: pd.DataFrame,\n",
    "    smooth_window: int = 7,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature builder for ball hit / bounce detection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Numeric positions and index\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df = df.copy()\n",
    "    new_df.index = pd.to_numeric(new_df.index, errors=\"coerce\")\n",
    "    new_df = new_df.sort_index()\n",
    "    new_df[\"x_i\"] = pd.to_numeric(new_df[\"x\"], errors=\"coerce\")\n",
    "    new_df[\"y_i\"] = pd.to_numeric(new_df[\"y\"], errors=\"coerce\")\n",
    "    new_df = new_df.dropna(subset=[\"x_i\", \"y_i\"])\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw positions\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df[\"x_raw\"] = new_df[\"x_i\"]\n",
    "    new_df[\"y_raw\"] = new_df[\"y_i\"]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Centered smoothing on positions\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Centered rolling mean reduces high-frequency measurement noise\n",
    "    # without eliminating physical discontinuities (hits / bounces).\n",
    "    new_df[\"x_smooth\"] = (\n",
    "        new_df[\"x_raw\"]\n",
    "        .rolling(smooth_window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "    new_df[\"y_smooth\"] = (\n",
    "        new_df[\"y_raw\"]\n",
    "        .rolling(smooth_window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Time step (central)\n",
    "    # ------------------------------------------------------------------\n",
    "    t = new_df.index.to_series()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Smoothed derivatives (stable kinematics)\n",
    "    # ------------------------------------------------------------------\n",
    "    x_smooth = new_df[\"x_smooth\"].to_numpy()\n",
    "    y_smooth = new_df[\"y_smooth\"].to_numpy()\n",
    "\n",
    "    vx = np.gradient(x_smooth, t)\n",
    "    vy = np.gradient(y_smooth, t)\n",
    "\n",
    "    ax = np.gradient(vx, t)\n",
    "    ay = np.gradient(vy, t)\n",
    "\n",
    "    jx = np.gradient(ax, t)\n",
    "    jy = np.gradient(ay, t)\n",
    "\n",
    "    new_df[\"vx\"] = vx\n",
    "    new_df[\"vy\"] = vy\n",
    "    new_df[\"ax\"] = ax\n",
    "    new_df[\"ay\"] = ay\n",
    "    new_df[\"jx\"] = jx\n",
    "    new_df[\"jy\"] = jy\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw derivatives (impulse-sensitive)\n",
    "    # ------------------------------------------------------------------\n",
    "    x_raw = new_df[\"x_raw\"].to_numpy()\n",
    "    y_raw = new_df[\"y_raw\"].to_numpy()\n",
    "\n",
    "    vx_raw = np.gradient(x_raw, t)\n",
    "    vy_raw = np.gradient(y_raw, t)\n",
    "\n",
    "    ax_raw = np.gradient(vx_raw, t)\n",
    "    ay_raw = np.gradient(vy_raw, t)\n",
    "\n",
    "    jx_raw = np.gradient(ax_raw, t)\n",
    "    jy_raw = np.gradient(ay_raw, t)\n",
    "\n",
    "    new_df[\"vx_raw\"] = vx_raw\n",
    "    new_df[\"vy_raw\"] = vy_raw\n",
    "    new_df[\"ax_raw\"] = ax_raw\n",
    "    new_df[\"ay_raw\"] = ay_raw\n",
    "    new_df[\"jx_raw\"] = jx_raw\n",
    "    new_df[\"jy_raw\"] = jy_raw\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Raw derivatubes in absolute\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    new_df[\"vx_abs_raw\"] = np.abs(new_df[\"vx_raw\"])\n",
    "    new_df[\"vy_abs_raw\"] = np.abs(new_df[\"vy_raw\"])\n",
    "    new_df[\"ax_abs_raw\"] = np.abs(new_df[\"ax_raw\"])\n",
    "    new_df[\"ay_abs_raw\"] = np.abs(new_df[\"ay_raw\"])\n",
    "    new_df[\"jx_abs_raw\"] = np.abs(new_df[\"jx_raw\"])\n",
    "    new_df[\"jy_abs_raw\"] = np.abs(new_df[\"jy_raw\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Magnitudes (smoothed)\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df[\"v\"] = np.sqrt(new_df[\"vx\"]**2 + new_df[\"vy\"]**2)\n",
    "    new_df[\"a\"] = np.sqrt(new_df[\"ax\"]**2 + new_df[\"ay\"]**2)\n",
    "    new_df[\"jerk\"] = np.sqrt(new_df[\"jx\"]**2 + new_df[\"jy\"]**2)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Log magnitudes : preserves order and compresses large values\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df[\"log_v\"] = np.log1p(new_df[\"v\"])    \n",
    "    new_df[\"log_a\"] = np.log1p(new_df[\"a\"])\n",
    "    new_df[\"log_j\"] = np.log1p(new_df[\"jerk\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Directional features\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df[\"angle\"] = np.arctan2(new_df[\"vy\"], new_df[\"vx\"])\n",
    "    new_df[\"delta_angle\"] = np.gradient(new_df[\"angle\"])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Centered rolling statistics (smoothed)\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df[\"v_mean\"] = new_df[\"v\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    new_df[\"v_std\"]  = new_df[\"v\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    new_df[\"a_mean\"] = new_df[\"a\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    new_df[\"a_std\"]  = new_df[\"a\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    new_df[\"j_mean\"] = new_df[\"jerk\"].rolling(smooth_window, center=True, min_periods=1).mean()\n",
    "    new_df[\"j_std\"]  = new_df[\"jerk\"].rolling(smooth_window, center=True, min_periods=1).std().fillna(0)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Motion sign changes\n",
    "    # ------------------------------------------------------------------\n",
    "    new_df[\"vx_sign\"] = np.sign(new_df[\"vx\"]).fillna(0.0)\n",
    "    new_df[\"vx_sign_change\"] = (\n",
    "        new_df[\"vx_sign\"].diff().abs() > 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    new_df[\"vy_sign\"] = np.sign(new_df[\"vy\"]).fillna(0.0)\n",
    "    new_df[\"vy_sign_change\"] = (\n",
    "        new_df[\"vy_sign\"].diff().abs() > 0\n",
    "    ).astype(int)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Select features\n",
    "FEATURE_COLS = [\n",
    "    \"delta_angle\",\n",
    "    \"vx_sign_change\", \n",
    "    \"vy_sign_change\",\n",
    "    \"v\", \"a\", \"jerk\",\n",
    "    \"vx\", 'vy', 'ax', 'ay', 'jx', 'jy',\n",
    "    \"v_mean\", \"v_std\",\n",
    "    \"a_mean\", \"a_std\",\n",
    "    \"j_mean\", \"j_std\",\n",
    "    \"log_v\", \"log_a\", \"log_j\",\n",
    "    \"vx_abs_raw\", \"vy_abs_raw\",\n",
    "    \"ax_abs_raw\", \"ay_abs_raw\",\n",
    "    \"jx_abs_raw\", \"jy_abs_raw\",\n",
    "]\n",
    "FEATURE_COLS_DEEP = FEATURE_COLS + [\"x_i\", \"y_i\"]\n",
    "SMOOTH_WINDOW = 7\n",
    "\n",
    "# Selecting df\n",
    "df_copy = df.copy()\n",
    "df_copy.index = pd.to_numeric(df_copy.index, errors=\"coerce\")\n",
    "df_copy = df_copy.sort_index()\n",
    "\n",
    "# Train-Test Split\n",
    "split_point = int(0.8 * len(df_copy))\n",
    "train_df_raw = df_copy.iloc[:split_point]\n",
    "test_df_raw  = df_copy.iloc[split_point:]\n",
    "\n",
    "# DataFrames with processed features\n",
    "train_df = build_features(train_df_raw, smooth_window=SMOOTH_WINDOW)\n",
    "test_df  = build_features(test_df_raw,  smooth_window=SMOOTH_WINDOW)\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "X_test  = test_df[FEATURE_COLS]\n",
    "X_train_deep = train_df[FEATURE_COLS_DEEP]\n",
    "X_test_deep  = test_df[FEATURE_COLS_DEEP]\n",
    "y_train = train_df[\"action\"].to_numpy()\n",
    "y_test  = test_df[\"action\"].to_numpy()\n",
    "\n",
    "# Scaling (fit on train, apply to test)\n",
    "scaler = StandardScaler()\n",
    "scaler_deep = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "X_train_deep_scaled = scaler_deep.fit_transform(X_train_deep)\n",
    "X_test_deep_scaled  = scaler_deep.transform(X_test_deep)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_labeled = le.fit_transform(y_train)\n",
    "y_test_labeled  = le.transform(y_test)\n",
    "classes = le.classes_\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Saving preprocessors\n",
    "preprocessors = {\n",
    "    \"scaler\": scaler,\n",
    "    \"scaler_deep\": scaler_deep,\n",
    "    \"label_encoder\": le\n",
    "}\n",
    "joblib.dump(preprocessors, \"preprocessors.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d32a3e",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_with_and_without_tolerance(y_true, y_pred, tolerance=2, use_labels=True):\n",
    "    \"\"\"\n",
    "    Event-level evaluation for temporal predictions with +/- tolerance.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if use_labels:\n",
    "        event_classes = [\"bounce\", \"hit\"]\n",
    "    else:\n",
    "        event_classes = [1, 2]\n",
    "\n",
    "    print(f\"\\nTemporal event evaluation (+/- {tolerance} frames)\")\n",
    "    print(\"-\" * 52)\n",
    "    print(f\"{'Event':<12} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10}\")\n",
    "    print(\"-\" * 52)\n",
    "\n",
    "    for event in event_classes:\n",
    "        true_indices = np.where(y_true == event)[0]\n",
    "        pred_indices = np.where(y_pred == event)[0]\n",
    "\n",
    "        # ---------- Recall ----------\n",
    "        matched_true = np.array([np.any(np.abs(pred_indices - t) <= tolerance) for t in true_indices])\n",
    "        recall = matched_true.sum() / len(true_indices) if len(true_indices) > 0 else 0.0\n",
    "\n",
    "        # ---------- Precision ----------\n",
    "        matched_pred = np.array([np.any(np.abs(true_indices - p) <= tolerance) for p in pred_indices])\n",
    "        precision = matched_pred.sum() / len(pred_indices) if len(pred_indices) > 0 else 0.0\n",
    "\n",
    "        # ---------- F1 ----------\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        event_name = str(event).capitalize() if use_labels else f\"Class {event}\"\n",
    "        print(f\"{event_name:<12} | {precision:>10.3f} | {recall:>10.3f} | {f1:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef770f12",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b39cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Compute adaptive thresholds\n",
    "# ==============================\n",
    "def calculate_thresholds(df_train):\n",
    "    \"\"\"\n",
    "    Compute dynamic thresholds from training data for event detection.\n",
    "    \"\"\"\n",
    "    thresholds = {\n",
    "        \"VERT_ACC_MIN\": np.percentile(df_train[\"ay_abs_raw\"].values, 70),\n",
    "        \"VERT_ACC_PROM\": np.percentile(df_train[\"ay_abs_raw\"].values, 85),\n",
    "        \"HORIZ_SPEED_DELTA\": np.percentile(np.abs(np.diff(df_train[\"vx\"].values)), 90),\n",
    "        \"HORIZ_SPEED_MIN\": np.percentile(df_train[\"vx_abs_raw\"].values, 10),\n",
    "        \"VERT_SPEED_MIN\": np.percentile(df_train[\"vy_abs_raw\"].values, 10),\n",
    "        \"VERT_ACC_BOUNCE\": np.percentile(df_train[\"ay_raw\"].values, 7),\n",
    "        \"JERK_THRESHOLD\": np.percentile(df_train[\"jerk\"].values, 90)\n",
    "    }\n",
    "    return thresholds\n",
    "\n",
    "# ==============================\n",
    "# Physics-based event detection\n",
    "# ==============================\n",
    "def detect_hits_and_bounces(df_test, thresholds, min_frames=10, window=2):\n",
    "    \"\"\"\n",
    "    Detect events (\"hit\" or \"bounce\") from motion trajectory data.\n",
    "    \"\"\"\n",
    "    df = df_test.copy()\n",
    "\n",
    "    # Raw signals\n",
    "    vert_acc = df[\"ay_raw\"].values\n",
    "    vert_acc_abs = df[\"ay_abs_raw\"].values\n",
    "    horiz_speed = df[\"vx_raw\"].values\n",
    "    vert_speed = df[\"vy_raw\"].values\n",
    "    jerk_vals = df[\"jerk\"].values\n",
    "\n",
    "    # Candidate event peaks\n",
    "    peaks, _ = find_peaks(\n",
    "        vert_acc_abs,\n",
    "        height=thresholds[\"VERT_ACC_MIN\"],\n",
    "        prominence=thresholds[\"VERT_ACC_PROM\"],\n",
    "        distance=3\n",
    "    )\n",
    "\n",
    "    candidate_events = []\n",
    "\n",
    "    for idx in peaks:\n",
    "        if idx < window or idx + window >= len(df):\n",
    "            continue\n",
    "\n",
    "        # Velocity states around event\n",
    "        vx_before, vx_after = horiz_speed[idx - window], horiz_speed[idx + window]\n",
    "        vy_before, vy_after = vert_speed[idx - window], vert_speed[idx + window]\n",
    "        ay_val = vert_acc[idx]\n",
    "        jerk_val = jerk_vals[idx]\n",
    "\n",
    "        # Derived physics metrics\n",
    "        horiz_flip = vx_before * vx_after < 0\n",
    "        vert_flip = vy_before * vy_after < 0\n",
    "        horiz_delta = abs(vx_after) - abs(vx_before)\n",
    "        vert_ratio = abs(vy_after) / (abs(vy_before) + 1e-6)\n",
    "        angle_before = np.arctan2(vy_before, vx_before)\n",
    "        angle_after = np.arctan2(vy_after, vx_after)\n",
    "        angle_change = abs(angle_after - angle_before)\n",
    "\n",
    "        # Event scoring\n",
    "        hit_points = 0.0\n",
    "        bounce_points = 0.0\n",
    "\n",
    "        # Hit scoring\n",
    "        hit_points += 2.0 if horiz_flip else 0.0 # vx changes direction\n",
    "        hit_points += 1.5 if horiz_delta > thresholds[\"HORIZ_SPEED_DELTA\"] else 0.0 # Magnitude of vx increases sharply in additional speed (not in ratio)\n",
    "        hit_points += 1.0 if vert_ratio > 1.1 else 0.0 # vy after the event increases by more than 10% (in magnitude)\n",
    "        hit_points += 1.0 if jerk_val > thresholds[\"JERK_THRESHOLD\"] else 0.0 # If the rate of change of acceleration is high\n",
    "        hit_points += 1.0 if angle_change > np.pi / 4 else 0.0 # If trajectory angle changes by more than 45°\n",
    "\n",
    "        # Bounce scoring\n",
    "        bounce_points += 2.0 if ay_val < thresholds[\"VERT_ACC_BOUNCE\"] else 0.0 # ay is very negative\n",
    "        bounce_points += 1.5 if vert_flip else 0.0 # vy changes direction\n",
    "        bounce_points += 1.0 if vert_ratio < 0.8 else 0.0 # vy after the event decrease by more than 20% (energy loss)\n",
    "\n",
    "        # Decide event type\n",
    "        event_type = None\n",
    "        if hit_points >= bounce_points and hit_points >= 2.5:\n",
    "            event_type = \"hit\"\n",
    "        elif bounce_points > hit_points and bounce_points >= 2.0:\n",
    "            event_type = \"bounce\"\n",
    "\n",
    "        # Computing strength score to help chosing between close events\n",
    "        strength_score = abs(ay_val) + jerk_val + abs(horiz_delta) + angle_change\n",
    "\n",
    "        if event_type:\n",
    "            candidate_events.append((df.index[idx], event_type, strength_score))\n",
    "\n",
    "    # ==============================\n",
    "    # Strongest Event Selection (to prevent from selecting event too close)\n",
    "    # ==============================\n",
    "    candidate_events.sort(key=lambda x: x[0])\n",
    "    final_events = {}\n",
    "\n",
    "    for frame_id, label, score in candidate_events:\n",
    "        if not final_events:\n",
    "            final_events[frame_id] = (label, score)\n",
    "            continue\n",
    "\n",
    "        last_frame = max(final_events.keys())\n",
    "        if frame_id - last_frame >= min_frames:\n",
    "            final_events[frame_id] = (label, score)\n",
    "        else:\n",
    "            # Keep the event with higher strength\n",
    "            if score > final_events[last_frame][1]:\n",
    "                final_events[last_frame] = (label, score)\n",
    "\n",
    "    # Return events by frame\n",
    "    return {frame: label for frame, (label, _) in final_events.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68645512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evluation \n",
    "thresh = calculate_thresholds(train_df)\n",
    "joblib.dump(thresh, \"thresholds_physics.joblib\")\n",
    "y_pred = detect_hits_and_bounces(test_df, thresholds=thresh)\n",
    "y_pred_array = np.array([\"air\"] * len(y_test), dtype=object)\n",
    "for frame, label in y_pred.items():\n",
    "    if frame in test_df.index:\n",
    "        idx = test_df.index.get_loc(frame)\n",
    "        y_pred_array[idx] = label\n",
    "\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred_array,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test, y_pred_array))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_array,\n",
    "    use_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a4870",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d2df2",
   "metadata": {},
   "source": [
    "## Suppress close events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppress_close_events(candidates, min_frames=3):\n",
    "    \"\"\"\n",
    "    Temporal suppression:\n",
    "    keep only the highest-probability event within a time window.\n",
    "    Suppression is class-aware (hit does not suppress bounce).\n",
    "    \"\"\"\n",
    "    candidates.sort(key=lambda x: x[0])  # sort by frame index\n",
    "    final_events = {}\n",
    "\n",
    "    for frame, label, proba in candidates:\n",
    "        if not final_events:\n",
    "            final_events[frame] = (label, proba)\n",
    "            continue\n",
    "\n",
    "        last_frame = max(final_events.keys())\n",
    "        last_label, last_proba = final_events[last_frame]\n",
    "\n",
    "        if frame - last_frame >= min_frames:\n",
    "            final_events[frame] = (label, proba)\n",
    "        else:\n",
    "            # suppress only if same class\n",
    "            if label == last_label and proba > last_proba:\n",
    "                final_events[last_frame] = (label, proba)\n",
    "\n",
    "    return final_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938a69a",
   "metadata": {},
   "source": [
    "## 1.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d65dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    class_weight=\"balanced\",   # to help with class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Time-aware CV to preserve order of the frames and a gap to avoid data leakage\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [40, 50, 60],\n",
    "    \"min_samples_split\": [6, 7],\n",
    "    \"min_samples_leaf\": [2, 3, 4],\n",
    "    \"max_features\": [\"sqrt\",\"log2\", None]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1_macro\", # Each class’s F1 contributes equally, to help with class imbalance\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Predict\n",
    "# -----------------------------\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)\n",
    "\n",
    "# Map class names to column indices\n",
    "class_index = {c: i for i, c in enumerate(best_rf.classes_)}\n",
    "predicted_proba = np.array([\n",
    "    y_proba[i, class_index[y_pred[i]]] for i in range(len(y_pred))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Create candidates list for suppression\n",
    "# -----------------------------\n",
    "X_test_copy = X_test.copy()\n",
    "X_test_copy[\"action\"] = y_pred\n",
    "X_test_copy[\"action_proba\"] = predicted_proba\n",
    "\n",
    "candidates = [\n",
    "    (frame, row[\"action\"], row[\"action_proba\"])\n",
    "    for frame, row in X_test_copy.iterrows()\n",
    "    if row[\"action\"] != \"air\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Temporal suppression: keep strongest events within 3 frames\n",
    "# -----------------------------\n",
    "final_events = suppress_close_events(candidates, min_frames=3)\n",
    "\n",
    "# Reset all frames to \"air\"\n",
    "X_test_copy[\"action\"] = \"air\"\n",
    "\n",
    "# Re-assign only the kept events\n",
    "for frame, (label, proba) in final_events.items():\n",
    "    X_test_copy.loc[frame, \"action\"] = label\n",
    "\n",
    "y_pred = X_test_copy[\"action\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Save final model\n",
    "# -----------------------------\n",
    "joblib.dump(best_rf, \"model/rf_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fcae3",
   "metadata": {},
   "source": [
    "## 1.2 Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1) Time-aware CV with a small gap to avoid centered-window bleed\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "# 2) Balanced RF (undersampling per tree)\n",
    "rf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3) Lean grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"precision\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=True\n",
    ")\n",
    "\n",
    "joblib.dump(best_rf, \"unused_models/rfus_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd92fff",
   "metadata": {},
   "source": [
    "## 2.1 XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfe838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights per class\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_labeled\n",
    ")\n",
    "\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights[y_train_labeled > 0] *= 5\n",
    "\n",
    "f05_scorer = make_scorer(\n",
    "    fbeta_score,\n",
    "    beta=0.5,\n",
    "    average=\"macro\"\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(classes),\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_estimators=400,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=SMOOTH_WINDOW // 2 + 1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 6],\n",
    "    \"learning_rate\": [0.03, 0.07],\n",
    "    \"subsample\": [0.7, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 1.0],\n",
    "    \"min_child_weight\": [1, 5],\n",
    "    \"gamma\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=f05_scorer,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(\n",
    "    X_train,\n",
    "    y_train_labeled,\n",
    "    sample_weight=sample_weights\n",
    ")\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "best_xgb = grid.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(best_xgb, \"unused_models/xgb_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2dd7ec",
   "metadata": {},
   "source": [
    "## 2.2 XG BOOST with Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b75d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "pipe = ImbPipeline(steps=[\n",
    "    (\"rus\", RandomUnderSampler(random_state=42)),  # undersampling de la majority class\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=SMOOTH_WINDOW // 2 + 1)\n",
    "\n",
    "param_grid = {\n",
    "    \"xgb__max_depth\": [3, 6],\n",
    "    \"xgb__learning_rate\": [0.05, 0.1],\n",
    "    \"xgb__subsample\": [0.7, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.7, 1.0],\n",
    "    \"xgb__min_child_weight\": [1, 5],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train_labeled)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "best_pipe = grid.best_estimator_\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(best_pipe, \"unused_models/xgbus_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0f77f",
   "metadata": {},
   "source": [
    "## 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a587dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Make a small validation split from the tail of train (chronological) ======\n",
    "val_ratio = 0.1\n",
    "split_idx = int((1.0 - val_ratio) * len(X_train_scaled))\n",
    "X_train_mlp, X_val_mlp = X_train_scaled[:split_idx], X_train_scaled[split_idx:]\n",
    "y_train_mlp, y_val_mlp = y_train_labeled[:split_idx], y_train_labeled[split_idx:]\n",
    "\n",
    "\n",
    "# --- Your model builder ---\n",
    "def build_mlp(input_dim, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[]  \n",
    "    )\n",
    "    return model\n",
    "\n",
    "mlp = build_mlp(X_train_mlp.shape[1], num_classes)\n",
    "\n",
    "# --- Macro F1 callback ---\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val, patience=5):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = -np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        f1_macro = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs = logs or {}\n",
    "        logs[\"val_f1_macro\"] = f1_macro\n",
    "        print(f\" — val_f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "        if f1_macro > self.best_f1:\n",
    "            self.best_f1 = f1_macro\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping on macro F1 (patience={self.patience}). Restoring best weights.\")\n",
    "                self.model.stop_training = True\n",
    "                if self.best_weights is not None:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "macro_f1_cb = MacroF1Callback(X_val_mlp, y_val_mlp, patience=5)\n",
    "\n",
    "# --- Other callbacks for stability ---\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    macro_f1_cb,\n",
    "]\n",
    "\n",
    "# Weights per class\n",
    "sample_weights_mlp = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_mlp\n",
    ")\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights_mlp[y_train_mlp > 0] *= 5\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train_mlp, y_train_mlp,\n",
    "    validation_data=(X_val_mlp, y_val_mlp),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    sample_weight=sample_weights_mlp,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# ====== Evaluate on test ======\n",
    "y_proba = mlp.predict(X_test_scaled, batch_size=256)\n",
    "y_pred  = y_proba.argmax(axis=1)\n",
    "\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_labeled, y_pred))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_labeled,\n",
    "    y_pred,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(mlp, \"unused_models/mlp_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb986f7",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b784b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Macro F1 callback -----\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val, patience=6):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = -np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val, verbose=0, batch_size=128)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        f1_macro = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        logs = logs or {}\n",
    "        logs[\"val_f1_macro\"] = f1_macro\n",
    "        print(f\" — val_f1_macro: {f1_macro:.4f}\")\n",
    "\n",
    "        if f1_macro > self.best_f1:\n",
    "            self.best_f1 = f1_macro\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping on macro F1 (patience={self.patience}). Restoring best weights.\")\n",
    "                self.model.stop_training = True\n",
    "                if self.best_weights is not None:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "def build_lstm(window_size, feature_dim, num_classes, bidirectional=False):\n",
    "    inputs = keras.Input(shape=(window_size, feature_dim))\n",
    "\n",
    "    x = inputs\n",
    "    if bidirectional:\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    else:\n",
    "        x = layers.LSTM(64, return_sequences=True)(x)\n",
    "        x = layers.LSTM(64)(x)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[]  \n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ====== Sequences (from your code) ======\n",
    "def make_sequences(X: np.ndarray, y: np.ndarray, window: int = 7, stride: int = 1):\n",
    "    X_seq, y_seq = [], []\n",
    "    for start in range(0, len(X) - window + 1, stride):\n",
    "        end = start + window\n",
    "        X_seq.append(X[start:end])\n",
    "        mid_idx = start + window // 2\n",
    "        y_seq.append(y[mid_idx])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "window = 7\n",
    "stride = 1\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train_deep_scaled, y_train_labeled, window=window, stride=stride)\n",
    "X_test_seq,  y_test_seq  = make_sequences(X_test_deep_scaled,  y_test_labeled,  window=window, stride=stride)\n",
    "\n",
    "num_classes = len(classes)\n",
    "feature_dim = X_train_seq.shape[-1]\n",
    "\n",
    "# Chronological validation split (tail)\n",
    "val_ratio = 0.1\n",
    "split_idx = int((1.0 - val_ratio) * len(X_train_seq))\n",
    "X_train_lstm, X_val_lstm = X_train_seq[:split_idx], X_train_seq[split_idx:]\n",
    "y_train_lstm, y_val_lstm = y_train_seq[:split_idx], y_train_seq[split_idx:]\n",
    "\n",
    "# Weights per class\n",
    "sample_weights_lstm = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=y_train_lstm\n",
    ")\n",
    "# Boost non-zero classes (hits / bounces)\n",
    "sample_weights_lstm[y_train_lstm > 0] *= 5\n",
    "\n",
    "# ====== Build LSTM ======\n",
    "lstm = build_lstm(window, feature_dim, num_classes, bidirectional=True)\n",
    "\n",
    "# Callbacks: LR on val_loss, early stop/restore on val macro F1 via custom callback\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_f1_macro\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    MacroF1Callback(X_val_lstm, y_val_lstm, patience=6),\n",
    "]\n",
    "\n",
    "# ====== Train ======\n",
    "history = lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_val_lstm, y_val_lstm),\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    sample_weight=sample_weights_lstm,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# ====== Evaluate ======\n",
    "y_proba_seq = lstm.predict(X_test_seq, batch_size=128)\n",
    "y_pred_seq  = y_proba_seq.argmax(axis=1)\n",
    "\n",
    "print(\"=\"*18 +\"Standard Evaluation\"+\"=\"*18)\n",
    "print(classification_report(\n",
    "    y_test_seq,\n",
    "    y_pred_seq,\n",
    "    target_names=classes,\n",
    "    zero_division=0\n",
    "))\n",
    "print(confusion_matrix(y_test_seq, y_pred_seq))\n",
    "\n",
    "print(\"=\"*13 +\"Temporal Tolerance Evaluation\"+\"=\"*13)\n",
    "evaluation_with_and_without_tolerance(\n",
    "    y_test_seq,\n",
    "    y_pred_seq,\n",
    "    tolerance=2,\n",
    "    use_labels=False\n",
    ")\n",
    "\n",
    "joblib.dump(lstm, \"unused_models/lstm_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hit_bounce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
